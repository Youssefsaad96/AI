{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications - (Advanced ‚ö°Ô∏è) üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Application\n",
    "This application reads data about Model Context Protocol (MCP) server from internet, stores in vector stores, chunks the data with embedding and useful to answer the question about MCP while inferenced.\n",
    "\n",
    "<img src=\"./img/RAG.png\" width=\"500\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model with your API key\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\",  # Replace with your actual API key\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the OpenAI model you want to use\n",
    "    temperature=0.5,        # Control randomness\n",
    "    max_tokens=250          # Limit the response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:latest\",\n",
    "    temperature=0.5,\n",
    "    max_tokens = 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP, or Model Context Protocol, is a protocol designed to enable AI assistants to interact with various external APIs and platforms. It supports actions like retrieving channel history from messaging apps and performing Git operations on GitHub. MCP servers, which include reference, official integrations, and community servers, demonstrate how different systems can integrate with this protocol to enhance their functionality with AI assistant capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üéâü•≥ Congratulations! You've successfully logged in! üôå \n",
       "</pre>\n"
      ],
      "text/plain": [
       "üéâü•≥ Congratulations! You've successfully logged in! üôå \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"fcG9Vm+mtQ6ms6JbQ3eRE4Dfiq2xXLX9QMQMtaLWz8A=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/karthik/tryout/aiqaDemo/myenv312/lib/python3.12/site-packages/deepeval/__init__.py:54: UserWarning: You are using deepeval version 2.5.9, however version 2.6.3 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "üôå Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is MCP',\n",
       "  'expected_output': 'The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'},\n",
       " {'input': 'What is Relationship between function calling & Model Context Protocol',\n",
       "  'expected_output': 'The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.'},\n",
       " {'input': 'What are the core components of MCP, just give the heading',\n",
       "  'expected_output': ' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                '}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"expected_output\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"expected_output\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"expected_output\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Golden(input='What is MCP', actual_output=None, expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None),\n",
       " Golden(input='What is Relationship between function calling & Model Context Protocol', actual_output=None, expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None),\n",
       " Golden(input='What are the core components of MCP, just give the heading', actual_output=None, expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden, EvaluationDataset\n",
    "\n",
    "goldens = []\n",
    "\n",
    "for data in test_data:\n",
    "    golden = Golden(\n",
    "        input=data['input'],\n",
    "        expected_output=data['expected_output']\n",
    "    )\n",
    "    \n",
    "    goldens.append(golden)\n",
    "  \n",
    "    \n",
    "\n",
    "dataset = EvaluationDataset(goldens=goldens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='What is MCP', actual_output=None, expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None), Golden(input='What is Relationship between function calling & Model Context Protocol', actual_output=None, expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None), Golden(input='What are the core components of MCP, just give the heading', actual_output=None, expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values=None)], conversational_goldens=[], _alias=None, _id=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\13735\\OneDrive - Expleo France\\Desktop\\Ai Testing course\\myenv312\\Lib\\site-packages\\rich\\live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\13735\\OneDrive - Expleo France\\Desktop\\Ai Testing course\\myenv312\\Lib\\site-packages\\rich\\live.py:231: \n",
       "UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.pull(alias=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(test_cases=[], goldens=[Golden(input='What is MCP', actual_output=None, expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values={}), Golden(input='What is Relationship between function calling & Model Context Protocol', actual_output=None, expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values={}), Golden(input='What are the core components of MCP, just give the heading', actual_output=None, expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, source_file=None, name=None, custom_column_key_values={})], conversational_goldens=[], _alias=test, _id=cmci1pado0b4eqroewil2z9f3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# It is going to use the LLM and Vector database stored information (RAG)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain(\"What is MCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\n",
      "\n",
      "cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\n",
      "\n",
      "history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\n",
      "\n",
      "history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\n"
     ]
    }
   ],
   "source": [
    "# Is the data which is stored in Vector DB\n",
    "retrieved_document = retrieve_and_format(\"What is MCP\")\n",
    "print(retrieved_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_context(question):\n",
    "    retrieved_document = retrieve_and_format(question)\n",
    "    response = qa_chain.run(question)\n",
    "    return retrieved_document, response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, context = query_with_context(\"What is MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LLMTestCase with Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def convert_goldens_to_test_cases(goldens: List[Golden]) -> List[LLMTestCase]:\n",
    "    test_cases = []\n",
    "    for golden in goldens:\n",
    "        context, rag_response = query_with_context(golden.input)\n",
    "        test_case = LLMTestCase(\n",
    "            input=golden.input,\n",
    "            actual_output=rag_response,\n",
    "            expected_output=golden.expected_output,\n",
    "            retrieval_context=[context],\n",
    "        )\n",
    "        test_cases.append(test_case)\n",
    "    return test_cases\n",
    "\n",
    "data = convert_goldens_to_test_cases(dataset.goldens)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMTestCase(input='What is MCP', actual_output='MCP stands for Messaging Control Platform. It is a platform that allows AI assistants to interact with external APIs, retrieve information from various sources, and perform a wide variety of actions, such as automating processes, extracting data, and building AI-based apps. The lack of standardization and fragmented implementation in MCP can sometimes lead to integration issues and inconsistencies across different developers and companies.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=['cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\ncutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP'], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None),\n",
       " LLMTestCase(input='What is Relationship between function calling & Model Context Protocol', actual_output=\"Function calling and Model Context Protocol (MCP) are related in the context of standardizing the way tools (functions) are specified and executed across AI systems. Function calling, like Custom GPTs using GPT Actions, allows AI apps to determine which API call resolves a user's prompt and make the necessary API calls. On the other hand, MCP defines a protocol for discovering available tools and executing them, creating a universal format for AI apps to use any tool without custom integration code. So, while function calling focuses on the specific actions within an AI app, MCP provides the structure and protocol for integrating and executing these functions within the broader AI system.\", expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=[\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\"], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None),\n",
       " LLMTestCase(input='What are the core components of MCP, just give the heading', actual_output='The core components of MCP are:\\n1. Standardization of tool specification and execution\\n2. Universal, plug-and-play format for AI app integration\\n3. Abstracting Docker engine management for tinkerers\\n4. Integration with CRM tools like HubSpot\\n5. Security considerations for MCP servers', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=[\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\"], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 3 test case(s) in parallel: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|100% (3/3) [Time Taken: 00:17,  5.83s/test case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question about MCP without any irrelevant information. Great job on staying focused and providing a clear answer!, error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!, error: None)\n",
      "  - ‚ùå Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input. The first node discusses the lack of standardization and fragmented implementation without mentioning MCP, which is not helpful in understanding what MCP is. Similarly, the second node repeats this information without providing any details about MCP. The third node mentions MCP's potential but fails to explain what MCP is or its purpose. Lastly, the fourth node reiterates MCP's potential without clarifying its function. This lack of relevant information results in a low score., error: None)\n",
      "  - ‚ùå Contextual Relevancy (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because while the retrieval context mentions MCP's potential to retrieve information and its role in AI interactions, it primarily focuses on unrelated topics like integration issues and GitHub capabilities, which do not directly define or explain what MCP is., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: MCP stands for Messaging Control Platform. It is a platform that allows AI assistants to interact with external APIs, retrieve information from various sources, and perform a wide variety of actions, such as automating processes, extracting data, and building AI-based apps. The lack of standardization and fragmented implementation in MCP can sometimes lead to integration issues and inconsistencies across different developers and companies.\n",
      "  - expected output: The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\n",
      "  - context: None\n",
      "  - retrieval context: ['cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\ncutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Precision: 33.33% pass rate\n",
      "Contextual Relevancy: 33.33% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 0.8, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.80 because the response accurately lists the core components of MCP, but includes an irrelevant mention of CRM tool integration, which is not directly related to the question., error: None)\n",
      "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!, error: None)\n",
      "  - ‚ùå Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input. The first node discusses standardizing processes and protocols for AI systems, which does not mention any core components of MCP like 'MCP Client', 'MCP Servers', 'Protocol Handshake', or 'Capability Discovery'. The second node repeats this information without addressing the specific components of MCP. The third node talks about Docker management and HubSpot integration, which are unrelated to the core components of MCP. Similarly, the fourth node repeats information about Docker management and HubSpot integration, which do not relate to the core components of MCP. As a result, all nodes are irrelevant, leading to a score of 0.00., error: None)\n",
      "  - ‚ùå Contextual Relevancy (score: 0.375, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.38 because the majority of the retrieval context focuses on aspects unrelated to the core components of MCP, such as AI app functionalities, HubSpot integration, and security considerations. However, the relevant statements do touch on defining tools, providing protocols, and creating universal formats, which are pertinent to the core components of MCP., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What are the core components of MCP, just give the heading\n",
      "  - actual output: The core components of MCP are:\n",
      "1. Standardization of tool specification and execution\n",
      "2. Universal, plug-and-play format for AI app integration\n",
      "3. Abstracting Docker engine management for tinkerers\n",
      "4. Integration with CRM tools like HubSpot\n",
      "5. Security considerations for MCP servers\n",
      "  - expected output:  \n",
      "                    - MCP Client\n",
      "                    - MCP Servers\n",
      "                    - Protocol Handshake\n",
      "                    - Capability Discovery\n",
      "                \n",
      "  - context: None\n",
      "  - retrieval context: [\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Precision: 33.33% pass rate\n",
      "Contextual Relevancy: 33.33% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the relationship between function calling and Model Context Protocol without any irrelevant statements. Great job on staying focused and relevant!, error: None)\n",
      "  - ‚úÖ Faithfulness (score: 0.8, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.80 because the actual output incorrectly attributes the creation of a universal format for AI apps to MCP, whereas the retrieval context does not specifically credit MCP with this development., error: None)\n",
      "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because all relevant nodes are perfectly ranked at the top, providing comprehensive insights into the relationship between function calling and the Model Context Protocol. Each node effectively contributes to understanding the standardized processes, tool discovery, and communication layers, ensuring a seamless connection between AI applications and context., error: None)\n",
      "  - ‚úÖ Contextual Relevancy (score: 0.9, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.90 because the retrieval context provides several relevant statements that highlight the relationship between function calling and the Model Context Protocol. For instance, it mentions 'Defining a consistent way to specify tools (functions) across any AI system' and 'Providing a protocol for discovering available tools and executing them,' which directly relate to the input query. However, the incomplete statement 'MCP supports two primary' slightly detracts from the overall relevance, preventing a perfect score., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is Relationship between function calling & Model Context Protocol\n",
      "  - actual output: Function calling and Model Context Protocol (MCP) are related in the context of standardizing the way tools (functions) are specified and executed across AI systems. Function calling, like Custom GPTs using GPT Actions, allows AI apps to determine which API call resolves a user's prompt and make the necessary API calls. On the other hand, MCP defines a protocol for discovering available tools and executing them, creating a universal format for AI apps to use any tool without custom integration code. So, while function calling focuses on the specific actions within an AI app, MCP provides the structure and protocol for integrating and executing these functions within the broader AI system.\n",
      "  - expected output: The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\n",
      "  - context: None\n",
      "  - retrieval context: [\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Precision: 33.33% pass rate\n",
      "Contextual Relevancy: 33.33% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Tests finished üéâ! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-cases\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cases</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m‚úì\u001b[0m Tests finished üéâ! View results on \n",
       "\u001b]8;id=702299;https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-cases\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=702299;https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-cases\u001b\\\u001b[4;94mcases\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addressed the question about MCP without any irrelevant information. Great job on staying focused and providing a clear answer!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005325000000000001, verbose_logs='Statements:\\n[\\n    \"MCP stands for Messaging Control Platform.\",\\n    \"It is a platform that allows AI assistants to interact with external APIs.\",\\n    \"MCP can retrieve information from various sources.\",\\n    \"It can perform a wide variety of actions.\",\\n    \"Actions include automating processes, extracting data, and building AI-based apps.\",\\n    \"The lack of standardization in MCP can lead to integration issues.\",\\n    \"Fragmented implementation can cause inconsistencies across developers and companies.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.008530000000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"The lack of standardization can cause an integration to stop working if a tool or model is updated or deprecated.\",\\n    \"Fragmented implementation can lead to unpredictable or undesirable results.\",\\n    \"Different developers and companies may implement inconsistent integrations.\",\\n    \"Fragmentation can cause end user confusion or frustration.\",\\n    \"MCP can retrieve information from a wide variety of sources, including popular messaging apps.\",\\n    \"GitHub supports a wide variety of actions, including automating processes, extracting or analyzing data, and building AI-based apps.\",\\n    \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\"\\n] \\n \\nClaims:\\n[\\n    \"MCP stands for Messaging Control Platform.\",\\n    \"MCP allows AI assistants to interact with external APIs.\",\\n    \"MCP can retrieve information from various sources.\",\\n    \"MCP can perform a wide variety of actions, such as automating processes, extracting data, and building AI-based apps.\",\\n    \"The lack of standardization in MCP can lead to integration issues.\",\\n    \"Fragmented implementation in MCP can cause inconsistencies across different developers and companies.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input. The first node discusses the lack of standardization and fragmented implementation without mentioning MCP, which is not helpful in understanding what MCP is. Similarly, the second node repeats this information without providing any details about MCP. The third node mentions MCP's potential but fails to explain what MCP is or its purpose. Lastly, the fourth node reiterates MCP's potential without clarifying its function. This lack of relevant information results in a low score.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.006030000000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context discusses the lack of standardization and fragmented implementation in integrations, but does not mention MCP or its role in addressing these issues.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context repeats the same information about lack of standardization and fragmented implementation without providing details about MCP.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context mentions the potential of MCP to retrieve information from various sources but does not explain what MCP is or its purpose.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context repeats the same information about MCP\\'s potential without explaining what MCP is or its function.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.3333333333333333, reason=\"The score is 0.33 because while the retrieval context mentions MCP's potential to retrieve information and its role in AI interactions, it primarily focuses on unrelated topics like integration issues and GitHub capabilities, which do not directly define or explain what MCP is.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0062775, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement discusses integration issues and standardization, which are not directly related to explaining what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement focuses on fragmented implementation issues, not on defining or explaining MCP.\"\\n            },\\n            {\\n                \"statement\": \"This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement addresses user confusion due to inconsistent integrations, which does not explain what MCP is.\"\\n            },\\n            {\\n                \"statement\": \"MCP underscores the potential to retrieve information from a wide variety of sources, including popular messaging apps.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement is about GitHub\\'s capabilities, not directly about MCP.\"\\n            },\\n            {\\n                \"statement\": \"The current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is MCP', actual_output='MCP stands for Messaging Control Platform. It is a platform that allows AI assistants to interact with external APIs, retrieve information from various sources, and perform a wide variety of actions, such as automating processes, extracting data, and building AI-based apps. The lack of standardization and fragmented implementation in MCP can sometimes lead to integration issues and inconsistencies across different developers and companies.', expected_output='The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools‚Äîessentially a ‚Äúuniversal remote‚Äù for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.', context=None, retrieval_context=['cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\ncutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it‚Äôs important to understand that MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP'], additional_metadata=None), TestResult(name='test_case_2', success=False, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=0.8, reason='The score is 0.80 because the response accurately lists the core components of MCP, but includes an irrelevant mention of CRM tool integration, which is not directly related to the question.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005050000000000001, verbose_logs='Statements:\\n[\\n    \"The core components of MCP include standardization of tool specification and execution.\",\\n    \"MCP offers a universal, plug-and-play format for AI app integration.\",\\n    \"It abstracts Docker engine management for tinkerers.\",\\n    \"MCP integrates with CRM tools like HubSpot.\",\\n    \"Security considerations are included for MCP servers.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"Integration with CRM tools is not a core component of MCP.\"\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.008467500000000001, verbose_logs='Truths (limit=None):\\n[\\n    \"There is a process that standardizes the specification of tools across AI systems.\",\\n    \"A protocol exists for discovering and executing available tools in AI systems.\",\\n    \"A universal format allows AI apps to use any tool without custom integration code.\",\\n    \"Custom GPTs use function calling, such as GPT Actions, to determine API calls.\",\\n    \"Custom GPTs can create necessary JSON for API calls.\",\\n    \"There is an abstraction of Docker management into a friendlier interface for tinkerers.\",\\n    \"HubSpot integration allows users to list and create contacts, get recent engagements, and manage companies.\",\\n    \"MCP servers use OAuth implementation with HTTP+SSE transport servers.\",\\n    \"MCP\\'s OAuth implementation exhibits the same risks as standard OAuth flows.\"\\n] \\n \\nClaims:\\n[\\n    \"The core components of MCP include standardization of tool specification and execution.\",\\n    \"MCP provides a universal, plug-and-play format for AI app integration.\",\\n    \"MCP abstracts Docker engine management for tinkerers.\",\\n    \"MCP integrates with CRM tools like HubSpot.\",\\n    \"Security considerations are part of MCP servers.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=False, score=0.0, reason=\"The score is 0.00 because all nodes in the retrieval contexts are irrelevant to the input. The first node discusses standardizing processes and protocols for AI systems, which does not mention any core components of MCP like 'MCP Client', 'MCP Servers', 'Protocol Handshake', or 'Capability Discovery'. The second node repeats this information without addressing the specific components of MCP. The third node talks about Docker management and HubSpot integration, which are unrelated to the core components of MCP. Similarly, the fourth node repeats information about Docker management and HubSpot integration, which do not relate to the core components of MCP. As a result, all nodes are irrelevant, leading to a score of 0.00.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.006685, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context discusses standardizing processes and protocols for AI systems, but does not mention any of the core components of MCP like \\'MCP Client\\', \\'MCP Servers\\', \\'Protocol Handshake\\', or \\'Capability Discovery\\'.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context repeats the same information about standardizing processes and protocols for AI systems, without mentioning the specific components of MCP.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context talks about Docker management and HubSpot integration, which are unrelated to the core components of MCP.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The context repeats information about Docker management and HubSpot integration, which do not relate to the core components of MCP.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=False, score=0.375, reason='The score is 0.38 because the majority of the retrieval context focuses on aspects unrelated to the core components of MCP, such as AI app functionalities, HubSpot integration, and security considerations. However, the relevant statements do touch on defining tools, providing protocols, and creating universal formats, which are pertinent to the core components of MCP.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.008127500000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Defining a consistent way to specify tools (functions) across any AI system.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Providing a protocol for discovering available tools and executing them.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions\\' does not directly relate to the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"A Custom GPT can determine which API call resolves the user\\'s prompt, create the necessary JSON, then make the API.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'A Custom GPT can determine which API call resolves the user\\'s prompt, create the necessary JSON, then make the API\\' is more about the functionality of Custom GPTs rather than the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies\\' is about HubSpot integration, not the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"Security considerations for MCP servers.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'Security considerations for MCP servers\\' is about security aspects, not the core components of MCP.\"\\n            },\\n            {\\n                \"statement\": \"MCP\\\\u2019s OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'MCP\\\\u2019s OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows\\' is about security risks, not the core components of MCP.\"\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What are the core components of MCP, just give the heading', actual_output='The core components of MCP are:\\n1. Standardization of tool specification and execution\\n2. Universal, plug-and-play format for AI app integration\\n3. Abstracting Docker engine management for tinkerers\\n4. Integration with CRM tools like HubSpot\\n5. Security considerations for MCP servers', expected_output=' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                ', context=None, retrieval_context=[\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP‚Äôs OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers\"], additional_metadata=None), TestResult(name='test_case_1', success=True, metrics_data=[MetricData(name='Answer Relevancy', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addresses the relationship between function calling and Model Context Protocol without any irrelevant statements. Great job on staying focused and relevant!', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.006072500000000001, verbose_logs='Statements:\\n[\\n    \"Function calling and Model Context Protocol (MCP) are related in standardizing tool specification and execution across AI systems.\",\\n    \"Function calling allows AI apps to determine which API call resolves a user\\'s prompt.\",\\n    \"Custom GPTs use GPT Actions for function calling.\",\\n    \"Function calling enables making necessary API calls.\",\\n    \"MCP defines a protocol for discovering available tools and executing them.\",\\n    \"MCP creates a universal format for AI apps to use any tool without custom integration code.\",\\n    \"Function calling focuses on specific actions within an AI app.\",\\n    \"MCP provides the structure and protocol for integrating and executing functions within the broader AI system.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Faithfulness', threshold=0.5, success=True, score=0.8, reason='The score is 0.80 because the actual output incorrectly attributes the creation of a universal format for AI apps to MCP, whereas the retrieval context does not specifically credit MCP with this development.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0102325, verbose_logs='Truths (limit=None):\\n[\\n    \"There is a process that standardizes the specification of tools across AI systems.\",\\n    \"A protocol exists for discovering and executing available tools in AI systems.\",\\n    \"A universal format allows AI apps to use any tool without custom integration code.\",\\n    \"Custom GPTs use function calling, such as GPT Actions.\",\\n    \"A Custom GPT can determine the appropriate API call for a user\\'s prompt.\",\\n    \"A Custom GPT can create the necessary JSON for API calls.\",\\n    \"The Model Context Protocol (MCP) is involved in the integration between host requirements and AI systems.\",\\n    \"Clients are integrated into host applications, such as the MCP client in Claude Desktop.\",\\n    \"MCP servers add context and capabilities to AI apps.\",\\n    \"Standalone MCP servers focus on specific integration points, like GitHub or PostgreSQL.\",\\n    \"The transport layer is the communication mechanism between clients and servers.\",\\n    \"MCP supports two primary communication mechanisms.\"\\n] \\n \\nClaims:\\n[\\n    \"Function calling and Model Context Protocol (MCP) are related in the context of standardizing the way tools (functions) are specified and executed across AI systems.\",\\n    \"Function calling, like Custom GPTs using GPT Actions, allows AI apps to determine which API call resolves a user\\'s prompt and make the necessary API calls.\",\\n    \"MCP defines a protocol for discovering available tools and executing them, creating a universal format for AI apps to use any tool without custom integration code.\",\\n    \"Function calling focuses on the specific actions within an AI app.\",\\n    \"MCP provides the structure and protocol for integrating and executing functions within the broader AI system.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The claim states that MCP creates a universal format for AI apps to use any tool without custom integration code, but the retrieval context attributes this to a universal format, not specifically to MCP.\"\\n    },\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Precision', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because all relevant nodes are perfectly ranked at the top, providing comprehensive insights into the relationship between function calling and the Model Context Protocol. Each node effectively contributes to understanding the standardized processes, tool discovery, and communication layers, ensuring a seamless connection between AI applications and context.', strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.005980000000000001, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context explains how the Model Context Protocol (MCP) standardizes the process of specifying and executing functions, which is a key aspect of function calling.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context mentions that MCP provides a protocol for discovering and executing tools, which aligns with the expected output\\'s description of MCP leveraging function calling.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context describes the role of MCP servers in adding context and capabilities, which relates to the expected output\\'s mention of MCP connecting AI applications to context.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context discusses the transport layer and communication between clients and servers, which is relevant to the expected output\\'s focus on consistent API interactions across applications.\"\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=0.5, success=True, score=0.9, reason=\"The score is 0.90 because the retrieval context provides several relevant statements that highlight the relationship between function calling and the Model Context Protocol. For instance, it mentions 'Defining a consistent way to specify tools (functions) across any AI system' and 'Providing a protocol for discovering available tools and executing them,' which directly relate to the input query. However, the incomplete statement 'MCP supports two primary' slightly detracts from the overall relevance, preventing a perfect score.\", strict_mode=False, evaluation_model='gpt-4o', error=None, evaluation_cost=0.0073625, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Defining a consistent way to specify tools (functions) across any AI system.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Providing a protocol for discovering available tools and executing them.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"A Custom GPT can determine which API call resolves the user\\'s prompt, create the necessary JSON, then make the API.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Clients are built into host applications, like the MCP client inside Claude Desktop.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"Transport layer: The communication mechanism between clients and servers.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            },\\n            {\\n                \"statement\": \"MCP supports two primary.\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'MCP supports two primary\\' is incomplete and does not provide relevant information about the relationship between function calling and the Model Context Protocol.\"\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is Relationship between function calling & Model Context Protocol', actual_output=\"Function calling and Model Context Protocol (MCP) are related in the context of standardizing the way tools (functions) are specified and executed across AI systems. Function calling, like Custom GPTs using GPT Actions, allows AI apps to determine which API call resolves a user's prompt and make the necessary API calls. On the other hand, MCP defines a protocol for discovering available tools and executing them, creating a universal format for AI apps to use any tool without custom integration code. So, while function calling focuses on the specific actions within an AI app, MCP provides the structure and protocol for integrating and executing these functions within the broader AI system.\", expected_output='The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.', context=None, retrieval_context=[\"standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\\n\\nbetween the host‚Äôs requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary\"], additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmcbwdbvz0d0jojp1s0x11kaz/evaluation/test-runs/cmcx71s1202wklxp611yimdcv/test-cases')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepeval.metrics\n",
    "\n",
    "\n",
    "deepeval.evaluate(\n",
    "    data, \n",
    "    metrics= [\n",
    "        deepeval.metrics.AnswerRelevancyMetric(),\n",
    "        deepeval.metrics.FaithfulnessMetric(),\n",
    "        deepeval.metrics.ContextualPrecisionMetric(),\n",
    "        deepeval.metrics.ContextualRelevancyMetric()\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
