{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4b8c3e",
   "metadata": {},
   "source": [
    "#### Testing with RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8163bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop-Academy.com\\AppData\\Local\\Temp\\ipykernel_18116\\3736072312.py:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model with your API key\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"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\",  # Replace with your actual API key\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the OpenAI model you want to use\n",
    "    temperature=0.5,        # Control randomness\n",
    "    max_tokens=250          # Limit the response length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12848381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90be8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\laptop-academy.com\\desktop\\ai testing\\ai\\newenv\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "   ---------------------------------------- 0.0/187.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/187.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/187.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/187.3 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 30.7/187.3 kB 262.6 kB/s eta 0:00:01\n",
      "   -------- ------------------------------ 41.0/187.3 kB 217.9 kB/s eta 0:00:01\n",
      "   -------------- ------------------------ 71.7/187.3 kB 302.7 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 112.6/187.3 kB 409.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 153.6/187.3 kB 482.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 187.3/187.3 kB 514.7 kB/s eta 0:00:00\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77161285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58885862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop-Academy.com\\AppData\\Local\\Temp\\ipykernel_18116\\3999062005.py:32: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP is a standardized protocol that allows for consistent integration of tools and models across different AI systems. It simplifies the process of defining, discovering, and executing tools, making it easier for AI apps to use different functions without custom integration code.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"What is MCP\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175af3ec",
   "metadata": {},
   "source": [
    "### Testing rag application with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c762cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# It is going to use the LLM and Vector database stored information (RAG)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d729c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_context(question):\n",
    "    retrieved_document = retrieve_and_format(question)\n",
    "    response = qa_chain.run(question)\n",
    "    return retrieved_document, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9035244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it’s important to understand that MCP\n",
      "\n",
      "history, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\n",
      "\n",
      "use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works. Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP\n",
      "\n",
      "standardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\n",
      "MCP stands for Model Communication Protocol. It is a standardized protocol that defines a consistent way for AI systems to specify tools (functions), discover available tools, and execute them. MCP aims to simplify the integration process between different AI systems by providing a universal, plug-and-play format where any AI app can use any tool without the need for custom integration code.\n"
     ]
    }
   ],
   "source": [
    "actual, context = query_with_context(\"What is MCP\")\n",
    "print(actual)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3108ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is MCP',\n",
       "  'reference': 'The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'},\n",
       " {'input': 'What is Relationship between function calling & Model Context Protocol',\n",
       "  'reference': 'The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.'},\n",
       " {'input': 'What are the core components of MCP, just give the heading',\n",
       "  'reference': ' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                '}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"input\": \"What is MCP\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is Relationship between function calling & Model Context Protocol\",\n",
    "        \"reference\": \"The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are the core components of MCP, just give the heading\",\n",
    "        \"reference\":\"\"\" \n",
    "                    - MCP Client\n",
    "                    - MCP Servers\n",
    "                    - Protocol Handshake\n",
    "                    - Capability Discovery\n",
    "                \"\"\"\n",
    "    }\n",
    "]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d2ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_input': 'What is MCP',\n",
       "  'retrieved_contexts': ['MCP stands for Model Communication Protocol. It is a protocol that standardizes the way AI systems interact with external APIs, making it easier to integrate different tools and models into AI applications.'],\n",
       "  'response': \"cutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it’s important to understand that MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\\n\\nuse,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\",\n",
       "  'reference': 'The Model Context Protocol (MCP) addresses this challenge by providing a standardized way for LLMs to connect with external data sources and tools—essentially a “universal remote” for AI apps. Released by Anthropic as an open-source protocol, MCP builds on existing function calling by eliminating the need for custom integration between LLMs and other apps.'},\n",
       " {'user_input': 'What is Relationship between function calling & Model Context Protocol',\n",
       "  'retrieved_contexts': ['The Model Context Protocol (MCP) standardizes how function calling works with API features. Function calling is not mutually exclusive with MCP; instead, MCP defines a consistent way to specify tools (functions) across any AI system, provides a protocol for discovering available tools and executing them, and creates a universal, plug-and-play format where any AI app can use any tool without custom integration code. In essence, function calling is a part of the broader framework provided by MCP to streamline interactions with external APIs and models.'],\n",
       "  'response': \"use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP\\n\\nhistory, and more. While straightforward, this underscores the potential for MCP to retrieve information from a wide variety of sources, including popular messaging apps.GitHub: Supports a wide variety of actions, including automating processes (e.g., pushing code), extracting or analyzing data, and even building AI-based apps. While the launch version was limited, the current GitHub MCP server serves as a benchmark for how AI assistants can interact with external APIs. Official MCP\\n\\ncutting edge. The lack of standardization means an integration can potentially stop working because a tool or model is updated, or an old one is deprecated. Fragmented implementation: Different integrations may handle similar functions in totally unexpected ways, creating unpredictable or undesirable results. This fragmentation can lead to end user confusion or frustration as different developers and companies implement inconsistent integrations. However, it’s important to understand that MCP\\n\\nstandardizes this process by:Defining a consistent way to specify tools (functions) across any AI system.Providing a protocol for discovering available tools and executing them.Creating a universal, plug-and-play format where any AI app can use any tool without custom integration code.You might be familiar with AI apps that use function calling, like Custom GPTs using GPT Actions. A Custom GPT can determine which API call resolves the user's prompt, create the necessary JSON, then make the API\",\n",
       "  'reference': 'The Model Context Protocol (MCP) builds on top of function calling, a well-established feature that allows large language models (LLMs) to invoke predetermined functions based on user requests. MCP simplifies and standardizes the development process by connecting AI applications to context while leveraging function calling to make API interactions more consistent across different applications and model vendors.'},\n",
       " {'user_input': 'What are the core components of MCP, just give the heading',\n",
       "  'retrieved_contexts': ['The core components of MCP are:\\n1. Function schemas\\n2. Handlers\\n3. Implementations'],\n",
       "  'response': 'use,” function calling is not mutually exclusive with MCP; the new protocol simply standardizes how this API feature works.\\xa0Without MCP, when you use a function call directly with an LLM API, you need to:Define model-specific function schemas, which are JSON descriptions of the function, acceptable parameters, and what it returns.Implement handlers (the actual code that executes when a function is called) for those functions.Create different implementations for each model you support.MCP\\n\\nThis integration provides code exploration (i.e., reading the file), breakpoint setting, and terminal command execution through natural language.\\xa0Apify: Through the use of over 4,000 Apify Actors, enables a wide range of functions including a RAG (Retrieval Augmented Generation) web browser, data scraping across multiple platforms, and content crawling.Community MCP serversThe community-driven ecosystem exemplifies how standardization can accelerate adoption and creativity. The following\\n\\nincreasingly valuable options to make everyone’s lives better.As adoption continues to grow, we’ll likely see AI assistants that are not just more informed about a given context, but ones who are genuinely more helpful in managing and navigating the complex digital world around us.For more developer updates from the world of authentication, access control, and AI, subscribe to our blog or follow us on LinkedIn.Identity and auth news.  Straight to your inbox.SubscribeLiked what you saw?Check out\\n\\ntinkerers, this abstracts Docker both local and remote engine management into a friendlier interface.HubSpot: This integration with ubiquitous CRM HubSpot allows users to list and create contacts, get recent engagements, and manage companies. While simple, this server provides a simple way to retrieve information for use with other tools.Security considerations for MCP serversMCP’s OAuth implementation using HTTP+SSE transport servers exhibits the same risks as standard OAuth flows. Developers',\n",
       "  'reference': ' \\n                    - MCP Client\\n                    - MCP Servers\\n                    - Protocol Handshake\\n                    - Capability Discovery\\n                '}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=[]\n",
    "\n",
    "for question in test_data:\n",
    "    actual, context = query_with_context(question[\"input\"])\n",
    "\n",
    "    dataset.append({\n",
    "        \"user_input\": question[\"input\"],\n",
    "        \"retrieved_contexts\":[context],\n",
    "        \"response\": actual,\n",
    "        \"reference\": question[\"reference\"]\n",
    "    })\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5602504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|███▎      | 3/9 [00:02<00:03,  1.65it/s]Exception raised in Job[5]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  44%|████▍     | 4/9 [00:02<00:02,  2.05it/s]Exception raised in Job[8]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  56%|█████▌    | 5/9 [00:02<00:01,  2.85it/s]Exception raised in Job[2]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Exception raised in Job[1]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  78%|███████▊  | 7/9 [00:03<00:00,  3.13it/s]Exception raised in Job[4]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  89%|████████▉ | 8/9 [00:03<00:00,  2.87it/s]Exception raised in Job[7]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating: 100%|██████████| 9/9 [00:07<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 0.3333, 'noise_sensitivity(mode=relevant)': nan, 'faithfulness': nan}\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, NoiseSensitivity, Faithfulness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas import (EvaluationDataset, evaluate)\n",
    "\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
    "\n",
    "result = evaluate(dataset=evaluation_dataset, \n",
    "                  metrics=[LLMContextRecall(), \n",
    "                           NoiseSensitivity(),\n",
    "                           Faithfulness()],\n",
    "                  llm = evaluator_llm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64eb239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is MCP</td>\n",
       "      <td>[MCP stands for Model Communication Protocol. ...</td>\n",
       "      <td>cutting edge. The lack of standardization mean...</td>\n",
       "      <td>The Model Context Protocol (MCP) addresses thi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is Relationship between function calling ...</td>\n",
       "      <td>[The Model Context Protocol (MCP) standardizes...</td>\n",
       "      <td>use,” function calling is not mutually exclusi...</td>\n",
       "      <td>The Model Context Protocol (MCP) builds on top...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the core components of MCP, just give...</td>\n",
       "      <td>[The core components of MCP are:\\n1. Function ...</td>\n",
       "      <td>use,” function calling is not mutually exclusi...</td>\n",
       "      <td>\\n                    - MCP Client\\n         ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                                        What is MCP   \n",
       "1  What is Relationship between function calling ...   \n",
       "2  What are the core components of MCP, just give...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [MCP stands for Model Communication Protocol. ...   \n",
       "1  [The Model Context Protocol (MCP) standardizes...   \n",
       "2  [The core components of MCP are:\\n1. Function ...   \n",
       "\n",
       "                                            response  \\\n",
       "0  cutting edge. The lack of standardization mean...   \n",
       "1  use,” function calling is not mutually exclusi...   \n",
       "2  use,” function calling is not mutually exclusi...   \n",
       "\n",
       "                                           reference  context_recall  \\\n",
       "0  The Model Context Protocol (MCP) addresses thi...             0.0   \n",
       "1  The Model Context Protocol (MCP) builds on top...             1.0   \n",
       "2   \\n                    - MCP Client\\n         ...             0.0   \n",
       "\n",
       "   noise_sensitivity(mode=relevant)  faithfulness  \n",
       "0                               NaN           NaN  \n",
       "1                               NaN           NaN  \n",
       "2                               NaN           NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (newEnv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
